<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="1000">
<meta name="description" content="Edwin D. de Jong">

<title>Edwin D. de Jong</title>
<link rel="canonical" href="http://edwin-de-jong.github.io/">


<style>


body {
  font-family: "Lato", sans-serif;
}

.sidenav {
  height: 100%;
  width: 280px;
  position: fixed;
  z-index: 1;
  top: 0;
  left: 0;
  background-color: #ffffff;
  overflow-x: hidden;
  padding-left: 10px;
  padding-top: 20px;
}

.sidenav a {
  padding: 6px 8px 6px 16px;
  text-decoration: none;
  font-size: 25px;
  color: #818181;
  display: block;
}

.sidenav a:hover {
  color: #000000;
}

.main {
  margin-left: 280px; /* Same as the width of the sidenav */
  font-size: 28px; /* Increased text to enable scrolling */
  padding: 0px 10px;
}

@media screen and (max-height: 450px) {
  .sidenav {padding-top: 15px;}
  .sidenav a {font-size: 18px;}
}
</style>
</head>
<body>
  
  

<div class="sidenav">
  <img src="fig/edwin-de-jong.jpg" style="width: 100%" alt="photo">
  <h2>Edwin D. de Jong</h2>
  
 <br><br>

<a href="https://scholar.google.com/citations?hl=en&user=l9w80gcAAAAJ&view_op=list_works&sortby=pubdate">Publications: Google Scholar</a><br>

<a href="https://x.com/EdwinDdeJong">@EdwinDdeJong</a><br>

<a href="https://www.linkedin.com/in/edwin-d-de-jong-260930/">LinkedIn profile</a><br>
   
   
<a href="#home">Home</a>
<a href="#pathrna">Pathology + RNA FM</a>
    <a href="#pathFM">Towards Large-scale Pathology FMs</a>
    <a href="#robustness_analysis">FM Robustness analysis</a>
    <a href="#screenpoint">Breast Cancer Detection</a>
    <a href="#isl">Incremental Sequence Learning</a>
    <a href="#mnist1d">MNIST 1d stroke dataset</a>
</div>

<div class="main">

  <section id="home"></section>
  I'm a medical AI researcher with extensive experience who actively contributes to 
  the AI revolution in healthcare that is currently taking place. At Kaiko.ai 
  we train, evaluate and apply <a href="#pathFM">medical foundation models</a>. With the research team 
  at ScreenPoint Medical,
 we improved the <a href="#screenpoint">breast cancer detection</a> performance of ScreenPoint's Transpara product. One of the areas I 
 focused on is measuring and improving the robustness of medical AI models. I did my postdoc at Prof. Jordan Pollack's <a href="http://www.demo.cs.brandeis.edu/">DEMO Lab</a> at <a href="https://www.brandeis.edu/">Brandeis University</a>, 
 and my PhD at the <a href="https://ai.vub.ac.be/">VUB AI Lab</a>. Below are some examples of my work.

  <section id="pathrna"></section>
    <h2>Pathology + RNA Foundation Model</h2>
    <figure>
      <a href="pub/Pathology-RNA-poster-London-Genomics.pdf">
          <img src="fig/pathology-rna.png"  style="width: 100%" alt="Pathology-RNA poster">
        </a>
        <figcaption>
            Enhancing Pathology Foundation Models with Transcriptomics. Edwin D. de Jong, Mikhail Karasikov, Marharyta Kurban, Moritz Platscher,
            Marie Stettler, Fei Tang. Poster at the <a href="https://www.genomicsengland.co.uk/events/genomics-england-research-summit-2024">Genomics England Research Summit 2024</a>. 
            Full poster: <a href="pub/Pathology-RNA-poster-London-Genomics.pdf">poster.pdf</a>
        </figcaption>
      </figure>

    Pathology Foundation Models (FMs) such as <a href="https://www.nature.com/articles/s41591-024-02857-3">UNI</a>, <a href="https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2">Phikon</a>, <a href="https://arxiv.org/abs/2401.04079">RudolfV</a> and <a href="https://arxiv.org/abs/2309.07778">Virchow</a> learn to represent patterns in H&E pathology images using Self-Supervised Learning.
    To see whether pathology FMs can be enhanced using RNA data, we compared two approaches: finetuning a pretrained pathology FM by using RNA expression levels as a high-dimensional target vector vs.
using contrastive learning in a <a href="https://openai.com/index/clip/">CLIP</a> setup </li>. We found that both approaches can improve pathology FM performance, as evaluated on a selection of downstream tasks.
  </section>


    <section id="pathFM"></section>
    <h2>Towards Large-Scale Training of Pathology Foundation Models</h2>
    To prepare for large-scale pathology FM trainings, we trained several Vision Transformer models on <a href="https://www.cancer.gov/ccg/research/genome-sequencing/tcga">TCGA</a> using <a href="https://github.com/facebookresearch/dino">DINO</a> and <a href="https://dinov2.metademolab.com/">DINO-v2</a>. <br>
    <b>Paper: </b><a href="https://arxiv.org/abs/2404.15217">Towards Large-Scale Training of Pathology Foundation Models</a>. kaiko.ai, Nanne Aben, Edwin D. de Jong, Ioannis Gatopoulos, Nicolas Känzig, Mikhail Karasikov, Axel Lagré, Roman Moser, Joost van Doorn, Fei Tang.
  
</section>
  

<section id="robustness_analysis">

<h2>Foundation Model Robustness Analysis</h2>


<figure>
  <img src="fig/robustness.png" style="width: 100%" alt="Color-coded visualization of embedding space clustering">
</figure>

To analyze what representations a Foundation Model (FM) pre-trained on <a href="https://www.cancer.gov/ccg/research/genome-sequencing/tcga">TCGA</a> using <a href="https://github.com/facebookresearch/dino">DINO</a> learns,
we cluster the embedding vectors of TCGA patches using <a href="https://lvdmaaten.github.io/tsne/">t-SNE</a>. This results in clusters
 in the embedding space. We then color these clusters in two ways: by disease (left) and 
 by the medical center from which the images originate (right). <br>

The coloring by disease shows that the FM has learned to distinguish different cancer types. 
The clustering on the right however shows that the overall organization of the embedding space
 is <i>also</i> strongly influenced by the medical center. This implies a risk for downstream trainings: the embeddings are not only determined by biological features such as 
the cancer type, but also by the center of origin, which should not play a role in downstream models and can lead to biases.
</section>

<section id="screenpoint">

    <h2>Breast Cancer Detection</h2>

With the research team at <a href="https://screenpoint-medical.com/">ScreenPoint Medical</a>,
we improved the breast cancer detection performance of ScreenPoint's Transpara product. 
The resulting model was evaluated in the large MASAI breast cancer randomized control trial 
in Denmark reported by Kristina Lång; see <a href="https://www.thelancet.com/journals/lanonc/article/PIIS1470-2045(23)00298-X/abstract">The Lancet article</a>. 
This study found that our AI product reduces radiologist workload by 44%, while 
detecting 20% more cancer cases, and was recognized by Nature Medicine 
as one of the <a href="https://portal.research.lu.se/en/prizes/notable-advances-2023-nature-medicine">Notable advances of 2023</a>. 
</section>


<section id="isl"></section>
<h2>Incremental Sequence Learning</h2>


<figure>
  <img src="fig/generative-rnn-training-movie.gif" style="width: 100%" alt="Incremental Sequence Learning"><br>
  <figcaption>Internal representations of digits learned by the model, shown over the course of training. See paper for details.
</figcaption>
</figure>


Recurrent Neural Networks (RNN) can learn to represent internal states that facilitate prediction. This work explores the intuition that
first learning short sequences makes it easier for the model to learn longer sequences. <br>

<b>NIPS 2016 Workshop paper:</b> <a href="https://arxiv.org/abs/1611.03068">Incremental Sequence Learning</a> (extended version)<br>

Incremental Sequence Learning <a href="https://edwin-de-jong.github.io/blog/isl/incremental-sequence-learning">blog post</a>
</section>


<section id="mnist1d"></section>
<h2>MNIST 1D Stroke Dataset</h2>
<p><a href="blog/mnist-sequence-data/"><img src="fig/mnist1d.png" style="width: 100%"></a></p>
Handwritten digits and letters can naturally be represented as sequences of 1D strokes. To study sequence learning, the well-known MNIST
dataset was transformed into pen strokes that approximately reproduce the digits in the original dataset. Dataset description and download:

<a href="blog/mnist-sequence-data">MNIST stroke data</a> sequence learning data set: all 70000 MNIST handwritten digit images transformed to 1D stroke sequences.

</section>






   
</body>
</html> 
